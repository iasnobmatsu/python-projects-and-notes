{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Disclaimer: The following code for scraping MAL was written on Dec 30th, 2020. The code is not garanteed to work if after the stated time, website structures for MAL changed. I will make an effort to update the code as often as possible. However, I did find being able to write the following code allows me to scrape most websites I want. \n",
    "\n",
    "### Scraping Static HTML: Using MAL Top Animes as An Example\n",
    "\n",
    "#### Import libraries\n",
    "\n",
    "- BeautifulSoup: for scraping\n",
    "- requests: request html and parse\n",
    "- re: regular expression for string manipulation\n",
    "- pandas: convert data scraped into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to Parse One Anime Row\n",
    "\n",
    "Looking at the html of https://myanimelist.net/topanime.php (using chrome, right click and select inspect, navigate to the element section, and you will see the HTML), each anime is a tr (table row) of the table. Within each row, name of anime is wrapped in class anime_ranking_h3, related information in class information, and score in class score. These can be scraped with beautifulsoup rather simply using the select() function. Then the text can be cleaned.\n",
    "\n",
    "We can further get a show's start year and end year from the related information section. Here I used regular expression to get 4 digits of year to match start and end years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneRow(targetrow):\n",
    "    animeTitle=targetrow.select(\"h3.anime_ranking_h3\")[0].text\n",
    "    animeInformation=targetrow.select(\"div.information\")[0].text.replace(\"\\n\",\"|\").replace(\"  \",\"\")\n",
    "    animeScore=targetrow.select(\"td.score\")[0].text.replace(\"\\n\", \"\")\n",
    "    year=animeInformation.split(\"|\") # split by |\n",
    "    years=re.findall('[0-9]+', year[2]); # get all years in the second section from above\n",
    "    start=\"NA\"\n",
    "    end=\"NA\"\n",
    "    \n",
    "    if len(year)>0:\n",
    "        start=years[0]\n",
    "        if len(years)>1:\n",
    "            end=years[1]\n",
    "    return animeTitle, animeInformation,animeScore, start, end\n",
    "\n",
    "# tablerow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Get a Specified Number of Anime on The Top Anime List\n",
    "\n",
    "Pass in the url into requests.get() function to get the entire page, then make a soup out of it with BeautifulSoup. With the soup ready, we could find the table corresponding to the top anime list and find all its rows. For each row, get desired data with the getOneRow() helper function. Because each page of the top anime list only has 50 animes, if requesting more than 50 anime, make sure to get a loop to scrape pages after the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopAnime(limit):\n",
    "    topanimedict=[] # I find using a dict to store data is the easiest, and it's easy to convert to JSON or csv\n",
    "    url = \"https://myanimelist.net/topanime.php\" #url\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'lxml') #make soup of html\n",
    "    toptable = soup.select(\"table\")[0] #get table corresponding to the top anime table.\n",
    "    tablerow=toptable.select(\"tr.ranking-list\") #get all rows in the table\n",
    "    for row in tablerow: #get data for each row\n",
    "        anime, info, score, st, ed=getOneRow(row)\n",
    "        tempdict={\"anime\": anime,\"start\": st, \"end\":ed,  \"score\": score, \"information\": info}\n",
    "        topanimedict.append(tempdict)\n",
    "        \n",
    "    if limit>50: # get page 2, 3, 4 etc after the first one\n",
    "        ind=limit//50\n",
    "        for i in range (1,ind):\n",
    "            url = \"https://myanimelist.net/topanime.php?limit=\"+str(50*i)\n",
    "            print(url)\n",
    "            soup = BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "            toptable = soup.select(\"table\")[0]\n",
    "            tablerow=toptable.select(\"tr.ranking-list\")\n",
    "            for row in tablerow:\n",
    "                anime, info, score, st, ed=getOneRow(row)\n",
    "                tempdict={\"anime\": anime,\"start\": st, \"end\":ed,  \"score\": score, \"information\": info}\n",
    "                topanimedict.append(tempdict)\n",
    "    \n",
    "    topanimedf=pd.DataFrame.from_dict(topanimedict)\n",
    "    return topanimedf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data\n",
    "\n",
    "With the help of a dictionary and the pandas library, it is really easy to convert what we scraped into a csv. This script will save the data to the same directory where the script is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/topanime.php?limit=50\n",
      "https://myanimelist.net/topanime.php?limit=100\n",
      "https://myanimelist.net/topanime.php?limit=150\n",
      "https://myanimelist.net/topanime.php?limit=200\n",
      "https://myanimelist.net/topanime.php?limit=250\n",
      "https://myanimelist.net/topanime.php?limit=300\n",
      "https://myanimelist.net/topanime.php?limit=350\n",
      "https://myanimelist.net/topanime.php?limit=400\n",
      "https://myanimelist.net/topanime.php?limit=450\n",
      "https://myanimelist.net/topanime.php?limit=500\n",
      "https://myanimelist.net/topanime.php?limit=550\n",
      "https://myanimelist.net/topanime.php?limit=600\n",
      "https://myanimelist.net/topanime.php?limit=650\n",
      "https://myanimelist.net/topanime.php?limit=700\n",
      "https://myanimelist.net/topanime.php?limit=750\n",
      "https://myanimelist.net/topanime.php?limit=800\n",
      "https://myanimelist.net/topanime.php?limit=850\n",
      "https://myanimelist.net/topanime.php?limit=900\n",
      "https://myanimelist.net/topanime.php?limit=950\n",
      "https://myanimelist.net/topanime.php?limit=1000\n",
      "https://myanimelist.net/topanime.php?limit=1050\n",
      "https://myanimelist.net/topanime.php?limit=1100\n",
      "https://myanimelist.net/topanime.php?limit=1150\n",
      "https://myanimelist.net/topanime.php?limit=1200\n",
      "https://myanimelist.net/topanime.php?limit=1250\n",
      "https://myanimelist.net/topanime.php?limit=1300\n",
      "https://myanimelist.net/topanime.php?limit=1350\n",
      "https://myanimelist.net/topanime.php?limit=1400\n",
      "https://myanimelist.net/topanime.php?limit=1450\n",
      "https://myanimelist.net/topanime.php?limit=1500\n",
      "https://myanimelist.net/topanime.php?limit=1550\n",
      "https://myanimelist.net/topanime.php?limit=1600\n",
      "https://myanimelist.net/topanime.php?limit=1650\n",
      "https://myanimelist.net/topanime.php?limit=1700\n",
      "https://myanimelist.net/topanime.php?limit=1750\n",
      "https://myanimelist.net/topanime.php?limit=1800\n",
      "https://myanimelist.net/topanime.php?limit=1850\n",
      "https://myanimelist.net/topanime.php?limit=1900\n",
      "https://myanimelist.net/topanime.php?limit=1950\n",
      "https://myanimelist.net/topanime.php?limit=2000\n",
      "https://myanimelist.net/topanime.php?limit=2050\n",
      "https://myanimelist.net/topanime.php?limit=2100\n",
      "https://myanimelist.net/topanime.php?limit=2150\n",
      "https://myanimelist.net/topanime.php?limit=2200\n",
      "https://myanimelist.net/topanime.php?limit=2250\n",
      "https://myanimelist.net/topanime.php?limit=2300\n",
      "https://myanimelist.net/topanime.php?limit=2350\n",
      "https://myanimelist.net/topanime.php?limit=2400\n",
      "https://myanimelist.net/topanime.php?limit=2450\n",
      "https://myanimelist.net/topanime.php?limit=2500\n",
      "https://myanimelist.net/topanime.php?limit=2550\n",
      "https://myanimelist.net/topanime.php?limit=2600\n",
      "https://myanimelist.net/topanime.php?limit=2650\n",
      "https://myanimelist.net/topanime.php?limit=2700\n",
      "https://myanimelist.net/topanime.php?limit=2750\n",
      "https://myanimelist.net/topanime.php?limit=2800\n",
      "https://myanimelist.net/topanime.php?limit=2850\n",
      "https://myanimelist.net/topanime.php?limit=2900\n",
      "https://myanimelist.net/topanime.php?limit=2950\n"
     ]
    }
   ],
   "source": [
    "df=getTopAnime(3000)\n",
    "df.to_csv('MALtop3000.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the scrape data file. Looked pretty neat to me. Index is the ranking-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime</th>\n",
       "      <th>end</th>\n",
       "      <th>information</th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>Sekirei</td>\n",
       "      <td>2008</td>\n",
       "      <td>|TV (12 eps)|Jul 2008 - Sep 2008|320,922 members|</td>\n",
       "      <td>7.14</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Shin Atashin'chi</td>\n",
       "      <td>2016</td>\n",
       "      <td>|TV (26 eps)|Oct 2015 - Apr 2016|2,427 members|</td>\n",
       "      <td>7.14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Tantei Opera Milky Holmes Movie: Gyakushuu no ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>|Movie (1 eps)|Feb 2016 - Feb 2016|3,417 members|</td>\n",
       "      <td>7.14</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Tenchi Muyou! Manatsu no Eve</td>\n",
       "      <td>1997</td>\n",
       "      <td>|Movie (1 eps)|Aug 1997 - Aug 1997|13,514 memb...</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Tengen Toppa Gurren Lagann: Parallel Works</td>\n",
       "      <td>2008</td>\n",
       "      <td>|Music (8 eps)|Jun 2008 - Sep 2008|29,743 memb...</td>\n",
       "      <td>7.14</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  anime   end  \\\n",
       "2995                                            Sekirei  2008   \n",
       "2996                                   Shin Atashin'chi  2016   \n",
       "2997  Tantei Opera Milky Holmes Movie: Gyakushuu no ...  2016   \n",
       "2998                       Tenchi Muyou! Manatsu no Eve  1997   \n",
       "2999         Tengen Toppa Gurren Lagann: Parallel Works  2008   \n",
       "\n",
       "                                            information score start  \n",
       "2995  |TV (12 eps)|Jul 2008 - Sep 2008|320,922 members|  7.14  2008  \n",
       "2996    |TV (26 eps)|Oct 2015 - Apr 2016|2,427 members|  7.14  2015  \n",
       "2997  |Movie (1 eps)|Feb 2016 - Feb 2016|3,417 members|  7.14  2016  \n",
       "2998  |Movie (1 eps)|Aug 1997 - Aug 1997|13,514 memb...  7.14  1997  \n",
       "2999  |Music (8 eps)|Jun 2008 - Sep 2008|29,743 memb...  7.14  2008  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Dynamic HTML: Using MAL user list as An Example¶\n",
    "\n",
    "with the code here, you will be able to scrape any user's MAL. Here I used my own anime list as an example (https://myanimelist.net/animelist/iasnobmatsu, fyi I highly highly recommend Attack on Titan, Haikyu, and Hoseki no Kuni).\n",
    "\n",
    "Dynamic HTML is different from static HTML as the static HTML is rendered from HTML source file (imaging writing an html file and that is what we scrape). Dynamic HTML, on the other side, is not rendered from HTML source files but from JavaScript (Or JQuery or React, whatever framework). Dynamic HTML, unlike static, is not generate the moment a url is opened, but will need some time to render after the document is ready.\n",
    "\n",
    "#### Helper Function to Get One Row of MAL User List\n",
    "\n",
    "Similar to the getOneRow function(), this function parses specific data for one anime. This step is the same regardless of static or dynamic HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Haikyuu!!', 'TV', '7', ' 25 ')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getOneRowMAL(targetrow):\n",
    "    animeTitle=targetrow.select(\"td.title\")[0].select(\"a.link.sort\")[0].text\n",
    "    animeType=targetrow.select(\"td.type\")[0].text.strip()\n",
    "    animeScore=targetrow.select(\"td.score\")[0].text.strip()\n",
    "    animeProgress=targetrow.select(\"td.progress\")[0].text.replace(\"\\n\", \"\").replace(\"  \",\"\")\n",
    "    return animeTitle, animeType,animeScore, animeProgress\n",
    "\n",
    "getOneRowMAL(rows[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Libraries for Dynamic HTML\n",
    "\n",
    "For scraping dynamic HTML, we need selenium and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Dynamic MAL User List Data\n",
    "\n",
    "to scrape dynamic data, we need the url of the webpage. We also need to have a web browser driver. Here I use the Chrome driver (download here https://chromedriver.chromium.org/ or through homebrew etc). I stored it in my download folder, and I will need the path to the driver.\n",
    "\n",
    "With the url of webpage and path to browser driver ready, we will use selenium to declare a driver variable, and use it instead of requests to get the url.\n",
    "\n",
    "Then it is important to delay the rest of the function by some time, here I used .2 but it may differ depend on how fast a page loads on a specific device under specific internet conditions. This time allows dynamic HTML to render so we scrape the desired content instead of the intial script used to generate the HTML (which we cannot parse).\n",
    "Then similar steps to scrape each row of data from the user anime list using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMAL(url, driverPath):\n",
    "    MALdict=[]\n",
    "    driver = webdriver.Chrome(driverPath)\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(0.2) # may change\n",
    "    soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    toptable = soup.select(\"table\")[0]\n",
    "    rows=toptable.select(\"tbody.list-item\")\n",
    "    for row in rows:\n",
    "        ti,ty,sc,pr=getOneRowMAL(row)\n",
    "        MALdict.append({\"anime\":ti,\"type\":ty, \"score\":sc,\"progress\":pr})\n",
    "    return pd.DataFrame.from_dict(MALdict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data\n",
    "\n",
    "Here we use the function above to get dynamic HTML data from my MAL list (you can replace with any user's MAL list. The data is saved again to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime</th>\n",
       "      <th>progress</th>\n",
       "      <th>score</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JoJo no Kimyou na Bouken Part 3: Stardust Crus...</td>\n",
       "      <td>- / 24</td>\n",
       "      <td>8</td>\n",
       "      <td>TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One Piece</td>\n",
       "      <td>- / -</td>\n",
       "      <td>8</td>\n",
       "      <td>TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shingeki no Kyojin: The Final Season</td>\n",
       "      <td>- / 16</td>\n",
       "      <td>10</td>\n",
       "      <td>TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akagami no Shirayuki-hime</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bleach</td>\n",
       "      <td>366</td>\n",
       "      <td>7</td>\n",
       "      <td>TV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               anime progress score type\n",
       "0  JoJo no Kimyou na Bouken Part 3: Stardust Crus...  - / 24      8   TV\n",
       "1                                          One Piece   - / -      8   TV\n",
       "2               Shingeki no Kyojin: The Final Season  - / 16     10   TV\n",
       "3                          Akagami no Shirayuki-hime      12      5   TV\n",
       "4                                             Bleach     366      7   TV"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://myanimelist.net/animelist/iasnobmatsu'\n",
    "driverp=\"/Users/ziqianxu/Downloads/chromedriver\"\n",
    "df2=getMAL(url,driverp)\n",
    "df2.to_csv('iasnobmatsuMAL.csv', index=False)\n",
    "df2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
